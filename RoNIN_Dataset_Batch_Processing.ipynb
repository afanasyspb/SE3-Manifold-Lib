{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3994611b-3bdc-4a0c-b42f-04e667741d1f",
   "metadata": {},
   "source": [
    "# **RoNIN Dataset Preprocessing Utility**\n",
    "### **Batch Processing: HDF5 to Standardized CSV Format**\n",
    "\n",
    "**Author:** Ilya Afanasyev  \n",
    "**Project:** Manifold-based Navigation Framework: Filters & Observers \n",
    "\n",
    "### **Overview**\n",
    "\n",
    "This notebook automates the extraction and standardization of the **SFU RoNIN Dataset**, and performs a full batch conversion of all 35 trajectories, transforming raw HDF5 data into a structured CSV format compatible with modern navigation filters and observers.\n",
    "\n",
    "### **Key Functions**\n",
    "\n",
    "*   **Automated Batch Extraction**: Processes the entire seen_subjects_test_set.zip in one pass.\n",
    "*   **Sensor Synchronization**: Exports IMU, Magnetometer, and Visual Odometry (VO) data.\n",
    "*   **Ground Truth Standardization**: Converts Google Tango Pose data into a standardized `vo.csv` (Visual Odometry) format.\n",
    "*   **Metadata Management**: Preserves original `info.json` as `metadata.json` and generates `trajectory_info.json` for each sequence.\n",
    "*   **Cross-Platform Paths**: Uses pathlib for compatibility across Windows, Linux, and macOS.\n",
    "\n",
    "### **Dataset Reference**\n",
    "\n",
    "This utility is designed for the **RoNIN: Robust IMU Double Integration** dataset.  \n",
    "Source: [https://ronin.cs.sfu.ca/](https://ronin.cs.sfu.ca/)  \n",
    "*Note: The dataset provides pre-interpolated ground truth data aligned to the 200Hz IMU sampling rate.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b79ee-b0c8-4366-adb3-c09faa2d86ea",
   "metadata": {},
   "source": [
    "# Cell 1: Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c09c935-6e89-4802-b37b-9aa20b2e7cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\awx887908\\appdata\\local\\anaconda3\\lib\\site-packages (3.14.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\awx887908\\appdata\\local\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\awx887908\\appdata\\local\\anaconda3\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\awx887908\\appdata\\local\\anaconda3\\lib\\site-packages (from h5py) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\awx887908\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\awx887908\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\awx887908\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\awx887908\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\awx887908\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries for HDF5 processing and data management\n",
    "!pip install h5py pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9604a-58db-4093-bc8c-fd72104c015e",
   "metadata": {},
   "source": [
    "# Cell 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06957f23-faa9-49a3-bcba-b5daecbc3323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Environment initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import h5py\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm # For visual progress tracking\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ Environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606edd2-b5f8-494d-b7a8-cc30f0a6997e",
   "metadata": {},
   "source": [
    "# Cell 3: Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4383564c-fca6-4aeb-8593-82e7684747f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Source ZIP: D:\\test\\seen_subjects_test_set.zip\n",
      "ðŸ“ Output Folder: D:\\test\\RoNIN_Standardized_Dataset\n"
     ]
    }
   ],
   "source": [
    "# Path to the source ZIP file (Update this to your local path)\n",
    "ZIP_FILE_PATH = Path(r'D:\\test\\seen_subjects_test_set.zip')\n",
    "\n",
    "# Base directory for standardized output\n",
    "OUTPUT_BASE_DIR = Path(r'D:\\test\\RoNIN_Standardized_Dataset')\n",
    "\n",
    "# Temporary directory for ZIP extraction (will be cleaned up automatically)\n",
    "TEMP_EXTRACT_DIR = Path(r'D:\\test\\RoNIN_temp_extraction')\n",
    "\n",
    "# Ensure directories exist\n",
    "OUTPUT_BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TEMP_EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ Source ZIP: {ZIP_FILE_PATH}\")\n",
    "print(f\"ðŸ“ Output Folder: {OUTPUT_BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a1559-e3ab-4f9d-b429-fcd3eeba189e",
   "metadata": {},
   "source": [
    "# Cell 4: RoNIN Batch Processor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4228c0b-ec96-407a-9b00-cb838a8e7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoNINBatchProcessor:\n",
    "    \"\"\"\n",
    "    Handles extraction, conversion, and cleanup of the RoNIN dataset.\n",
    "    Fixed: Auto-detects trajectory IDs and pretty-prints JSON metadata.\n",
    "    \"\"\"\n",
    "    def __init__(self, zip_path, output_dir, temp_dir):\n",
    "        self.zip_path = Path(zip_path)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.temp_dir = Path(temp_dir)\n",
    "        self.csv_root = self.output_dir / \"csv_data\"\n",
    "        self.csv_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def extract_dataset(self):\n",
    "        \"\"\"Unpacks the ZIP file to the temporary directory.\"\"\"\n",
    "        print(f\"ðŸ“¦ Extracting {self.zip_path.name}... (This may take a few minutes)\")\n",
    "        if not self.zip_path.exists():\n",
    "            raise FileNotFoundError(f\"Source ZIP not found at {self.zip_path}\")\n",
    "            \n",
    "        with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.temp_dir)\n",
    "        \n",
    "        # Locate all trajectory folders containing data.hdf5\n",
    "        # This accurately identifies the subfolder name as the Trajectory ID\n",
    "        traj_dirs = sorted([p.parent for p in self.temp_dir.rglob('data.hdf5')])\n",
    "        print(f\"âœ“ Found {len(traj_dirs)} trajectories.\")\n",
    "        return traj_dirs\n",
    "\n",
    "    def convert_to_csv(self, traj_dir, traj_id):\n",
    "        \"\"\"Converts a single HDF5 trajectory to standardized CSVs with tidy JSON.\"\"\"\n",
    "        try:\n",
    "            hdf5_path = traj_dir / \"data.hdf5\"\n",
    "            info_path = traj_dir / \"info.json\"\n",
    "\n",
    "            # 1. Read HDF5 Data\n",
    "            with h5py.File(hdf5_path, 'r') as h5:\n",
    "                sync = h5['synced']\n",
    "                t = sync['time'][:]\n",
    "                acc, gyro, mag = sync['acce'][:], sync['gyro'][:], sync['magnet'][:]\n",
    "                \n",
    "                pose = h5['pose']\n",
    "                vo_pos = pose['tango_pos'][:]\n",
    "\n",
    "            # 2. Create Target Directory (Using the detected traj_id)\n",
    "            target_path = self.csv_root / traj_id\n",
    "            target_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # 3. Export Sensors (200Hz)\n",
    "            pd.DataFrame({\n",
    "                'time': t, 'ax': acc[:,0], 'ay': acc[:,1], 'az': acc[:,2],\n",
    "                'wx': gyro[:,0], 'wy': gyro[:,1], 'wz': gyro[:,2]\n",
    "            }).to_csv(target_path / 'imu.csv', index=False)\n",
    "\n",
    "            pd.DataFrame({\n",
    "                'time': t, 'mx': mag[:,0], 'my': mag[:,1], 'mz': mag[:,2]\n",
    "            }).to_csv(target_path / 'mag.csv', index=False)\n",
    "\n",
    "            pd.DataFrame({\n",
    "                'time': t, 'X': vo_pos[:,0], 'Y': vo_pos[:,1], 'Z': vo_pos[:,2]\n",
    "            }).to_csv(target_path / 'vo.csv', index=False)\n",
    "\n",
    "            # 4. Process Metadata (Tidy JSON formatting)\n",
    "            if info_path.exists():\n",
    "                with open(info_path, 'r') as f_in:\n",
    "                    metadata_content = json.load(f_in)\n",
    "                with open(target_path / 'metadata.json', 'w') as f_out:\n",
    "                    json.dump(metadata_content, f_out, indent=4)\n",
    "\n",
    "            # 5. Generate Trajectory Summary\n",
    "            summary = {\n",
    "                'trajectory_id': traj_id,\n",
    "                'imu_samples': len(t),\n",
    "                'duration_sec': round(float(t[-1] - t[0]), 3),\n",
    "                'processed_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            with open(target_path / 'trajectory_info.json', 'w') as f:\n",
    "                json.dump(summary, f, indent=4)\n",
    "\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {traj_id}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Extract -> Convert -> Cleanup.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Extraction\n",
    "        traj_dirs = self.extract_dataset()\n",
    "        \n",
    "        # Step 2: Batch Processing\n",
    "        success_count = 0\n",
    "        for traj_path in tqdm(traj_dirs, desc=\"Standardizing Dataset\"):\n",
    "            # The folder name (e.g. a000_11) is the ID\n",
    "            traj_id = traj_path.name\n",
    "            \n",
    "            if self.convert_to_csv(traj_path, traj_id):\n",
    "                success_count += 1\n",
    "\n",
    "        # Step 3: Cleanup\n",
    "        print(f\"ðŸ§¹ Cleaning up temporary files at {self.temp_dir}...\")\n",
    "        shutil.rmtree(self.temp_dir, ignore_errors=True)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"COMPLETION SUMMARY\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Total Trajectories Found: {len(traj_dirs)}\")\n",
    "        print(f\"Successfully Processed:   {success_count}\")\n",
    "        print(f\"Total Processing Time:    {total_time/60:.2f} minutes\")\n",
    "        print(f\"Output Location: {self.csv_root.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a8f8f3-99ea-4fc1-bd8d-abaf8acf0e0e",
   "metadata": {},
   "source": [
    "# Cell 5: Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f97315-4339-41de-8358-3717c1fd3493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Extracting seen_subjects_test_set.zip... (This may take a few minutes)\n",
      "âœ“ Found 35 trajectories.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce99d0acc6024ccb845b9e656a88f6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Standardizing Dataset:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning up temporary files at D:\\test\\RoNIN_temp_extraction...\n",
      "\n",
      "==================================================\n",
      "COMPLETION SUMMARY\n",
      "==================================================\n",
      "Total Trajectories Found: 35\n",
      "Successfully Processed:   35\n",
      "Total Processing Time:    1.18 minutes\n",
      "Output Location: D:\\test\\RoNIN_Standardized_Dataset\\csv_data\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the processor\n",
    "processor = RoNINBatchProcessor(\n",
    "    zip_path=ZIP_FILE_PATH,\n",
    "    output_dir=OUTPUT_BASE_DIR,\n",
    "    temp_dir=TEMP_EXTRACT_DIR\n",
    ")\n",
    "\n",
    "# Run the full batch processing pipeline\n",
    "processor.run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6d644-3f2b-439d-8e45-af4ae978e0a6",
   "metadata": {},
   "source": [
    "# Cell 6: Dataset Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b114dc-844e-4dc6-9072-fb450ae1f464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Success! CSV files are saved in the 'csv_data' directory.\n"
     ]
    }
   ],
   "source": [
    "# List processed folders to verify trajectory IDs\n",
    "csv_data_path = OUTPUT_BASE_DIR / \"csv_data\"\n",
    "if csv_data_path.exists():\n",
    "    processed_trajectories = sorted([f.name for f in csv_data_path.iterdir() if f.is_dir()])\n",
    "    print(f\"Total Processed Folders: {len(processed_trajectories)}\")\n",
    "    print(f\"First 5 Trajectory IDs: {processed_trajectories[:5]}\")\n",
    "    \n",
    "    # Check the tidy JSON of the first trajectory\n",
    "    sample_json = csv_data_path / processed_trajectories[0] / 'trajectory_info.json'\n",
    "    if sample_json.exists():\n",
    "        print(f\"\\nContent of {processed_trajectories[0]}/trajectory_info.json:\")\n",
    "        with open(sample_json, 'r') as f:\n",
    "            print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
